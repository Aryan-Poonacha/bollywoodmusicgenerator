{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this file has all the code used to create the older versions of the RNN\n",
    "\n",
    "#OLDER MODEL VERSIONS\n",
    "\n",
    "#see https://github.com/hedonistrh/bestekar/blob/master/LSTM_colab.ipynb for v0\n",
    "\n",
    "#v1  \n",
    "def RNN_model(max_len, no_of_values):\n",
    "    \n",
    "    input_midi = keras.Input((max_len, no_of_values))\n",
    "    \n",
    "    #LSTM 1 is halved for v3 from 1024 to 512\n",
    "    x = layers.LSTM(1024, return_sequences=True, unit_forget_bias=True)(input_midi)\n",
    "    #\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # compute importance/attention for each step\n",
    "    attention = layers.Dense(1, activation='tanh')(x)\n",
    "    attention = layers.Flatten()(attention)\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "    #copy layer 1 is halved for v3 from 1024 to 512\n",
    "    attention = layers.RepeatVector(1024)(attention)\n",
    "    attention = layers.Permute([2, 1])(attention)\n",
    "\n",
    "    multiplied = layers.Multiply()([x, attention])\n",
    "    sent_representation = layers.Dense(512)(multiplied)\n",
    "\n",
    "    #halved from 512 to 256 for v2 and v3\n",
    "    x = layers.Dense(256)(sent_representation)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.22)(x)\n",
    "\n",
    "    #halved from 512 to 256 for v2 and v3\n",
    "    x = layers.LSTM(256, return_sequences=True, unit_forget_bias=True)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.22)(x)\n",
    "\n",
    "\n",
    "    # compute importance for each step\n",
    "    attention = layers.Dense(1, activation='tanh')(x)\n",
    "    attention = layers.Flatten()(attention)\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "    #halved from 512 to 256 for v2 and v3\n",
    "    attention = layers.RepeatVector(256)(attention)\n",
    "    attention = layers.Permute([2, 1])(attention)\n",
    "\n",
    "    multiplied = layers.Multiply()([x, attention])\n",
    "    #halved from 256 to 128 for v2 and v3\n",
    "    sent_representation = layers.Dense(128)(multiplied)\n",
    "\n",
    "    #halved from 256 to 128 for v2 and v3\n",
    "    x = layers.Dense(128)(sent_representation)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.22)(x)\n",
    "\n",
    "    #lowest possible value of 128 for all versions\n",
    "    x = layers.LSTM(128, unit_forget_bias=True)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.22)(x)\n",
    "\n",
    "    #lowest possible value of 128 for all versions\n",
    "    x = layers.Dense(128, activation='softmax')(x) \n",
    "\n",
    "    model = Model(inputs = input_midi, outputs = x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2   \n",
    "def RNN_model(max_len, no_of_values):\n",
    "    \n",
    "    input_midi = keras.Input((max_len, no_of_values))\n",
    "    \n",
    "    #LSTM 1 is halved for v3 from 1024 to 512\n",
    "    x = layers.LSTM(512, return_sequences=True, unit_forget_bias=True)(input_midi)\n",
    "    #\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # compute importance/attention for each step\n",
    "    attention = layers.Dense(1, activation='tanh')(x)\n",
    "    attention = layers.Flatten()(attention)\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "    #copy layer 1 is halved for v3 from 1024 to 512\n",
    "    attention = layers.RepeatVector(512)(attention)\n",
    "    attention = layers.Permute([2, 1])(attention)\n",
    "\n",
    "    multiplied = layers.Multiply()([x, attention])\n",
    "    sent_representation = layers.Dense(512)(multiplied)\n",
    "\n",
    "    #halved from 512 to 256 for v2 and v3\n",
    "    x = layers.Dense(256)(sent_representation)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.22)(x)\n",
    "\n",
    "    #halved from 512 to 256 for v2 and v3\n",
    "    x = layers.LSTM(256, return_sequences=True, unit_forget_bias=True)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.22)(x)\n",
    "\n",
    "\n",
    "    # compute importance for each step\n",
    "    attention = layers.Dense(1, activation='tanh')(x)\n",
    "    attention = layers.Flatten()(attention)\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "    #halved from 512 to 256 for v2 and v3\n",
    "    attention = layers.RepeatVector(256)(attention)\n",
    "    attention = layers.Permute([2, 1])(attention)\n",
    "\n",
    "    multiplied = layers.Multiply()([x, attention])\n",
    "    #halved from 256 to 128 for v2 and v3\n",
    "    sent_representation = layers.Dense(128)(multiplied)\n",
    "\n",
    "    #halved from 256 to 128 for v2 and v3\n",
    "    x = layers.Dense(128)(sent_representation)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.22)(x)\n",
    "\n",
    "    #lowest possible value of 128 for all versions\n",
    "    x = layers.LSTM(128, unit_forget_bias=True)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.22)(x)\n",
    "\n",
    "    #lowest possible value of 128 for all versions\n",
    "    x = layers.Dense(128, activation='softmax')(x) \n",
    "\n",
    "    model = Model(inputs = input_midi, outputs = x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v3 \n",
    "def RNN_model(max_len, no_of_values):\n",
    "    \n",
    "    input_midi = keras.Input((max_len, no_of_values))\n",
    "    \n",
    "    #LSTM 1 is halved for v3 from 1024 to 512\n",
    "    x = layers.Bidirectional(layers.LSTM(512, return_sequences=True, unit_forget_bias=True))(input_midi)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # compute importance/attention for each step\n",
    "    attention = layers.Dense(1, activation='tanh')(x)\n",
    "    attention = layers.Flatten()(attention)\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "    #copy layer 1 is halved for v3 from 1024 to 512\n",
    "    attention = layers.RepeatVector(1024)(attention)\n",
    "    attention = layers.Permute([2, 1])(attention)\n",
    "\n",
    "    multiplied = layers.Multiply()([x, attention])\n",
    "    sent_representation = layers.Dense(512)(multiplied)\n",
    "\n",
    "    #halved from 512 to 256 for v2 and v3\n",
    "    x = layers.Dense(256)(sent_representation)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.22)(x)\n",
    "\n",
    "    #halved from 512 to 256 for v2 and v3\n",
    "    x = layers.Bidirectional(layers.LSTM(256, return_sequences=True, unit_forget_bias=True))(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.22)(x)\n",
    "\n",
    "\n",
    "    # compute importance for each step\n",
    "    attention = layers.Dense(1, activation='tanh')(x)\n",
    "    attention = layers.Flatten()(attention)\n",
    "    attention = layers.Activation('softmax')(attention)\n",
    "    #halved from 512 to 256 for v2 and v3\n",
    "    attention = layers.RepeatVector(512)(attention)\n",
    "    attention = layers.Permute([2, 1])(attention)\n",
    "\n",
    "    multiplied = layers.Multiply()([x, attention])\n",
    "    #halved from 256 to 128 for v2 and v3\n",
    "    sent_representation = layers.Dense(128)(multiplied)\n",
    "\n",
    "    #halved from 256 to 128 for v2 and v3\n",
    "    x = layers.Dense(128)(sent_representation)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.22)(x)\n",
    "\n",
    "    #lowest possible value of 128 for all versions\n",
    "    x = layers.LSTM(128, unit_forget_bias=True)(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(0.22)(x)\n",
    "\n",
    "    #lowest possible value of 128 for all versions\n",
    "    x = layers.Dense(128, activation='softmax')(x) \n",
    "\n",
    "    model = Model(inputs = input_midi, outputs = x)\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
